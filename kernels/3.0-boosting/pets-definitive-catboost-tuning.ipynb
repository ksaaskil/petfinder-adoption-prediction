{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pets: Definitive CatBoost parameter tuning\n",
    "We'll do parameter tuning for [CatBoost](https://catboost.ai/) algorithm using [Pandas](https://pandas.pydata.org/) and [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). I'm a huge fan of pipelines as they ensure that (1) one **never** needs to modify the training data in-place and (2) all preprocessing steps can be parametrized and therefore tuned with, e.g., `GridSearchCV` (see the [example](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html)).\n",
    "\n",
    "Comments on this notebook are most welcome!\n",
    "\n",
    "### TODO\n",
    "- [ ] More tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define imports and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import catboost\n",
    "import tensorflow as tf  # Just for checking if GPU is available :)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(20.0, 10.0))\n",
    "GPU_AVAILABLE = tf.test.is_gpu_available()\n",
    "QUADRATIC_WEIGHT_SCORER = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "print(\"GPU available:\", GPU_AVAILABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../input\"\n",
    "print(os.listdir(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description (copied from [competition description](https://www.kaggle.com/c/petfinder-adoption-prediction/data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\n",
    "In this competition you will predict the speed at which a pet is adopted, based on the petâ€™s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \n",
    "This is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n",
    "\n",
    "### File descriptions\n",
    "- train.csv - Tabular/text data for the training set\n",
    "- test.csv - Tabular/text data for the test set\n",
    "- sample_submission.csv - A sample submission file in the correct format\n",
    "- breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n",
    "- color_labels.csv - Contains ColorName for each ColorID\n",
    "- state_labels.csv - Contains StateName for each StateID\n",
    "\n",
    "### Data Fields\n",
    "- PetID - Unique hash ID of pet profile\n",
    "- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n",
    "- Type - Type of animal (1 = Dog, 2 = Cat)\n",
    "- Name - Name of pet (Empty if not named)\n",
    "- Age - Age of pet when listed, in months\n",
    "- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n",
    "- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n",
    "- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n",
    "- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n",
    "- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n",
    "- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n",
    "- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n",
    "- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n",
    "- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n",
    "- Quantity - Number of pets represented in profile\n",
    "- Fee - Adoption fee (0 = Free)\n",
    "- State - State location in Malaysia (Refer to StateLabels dictionary)\n",
    "- RescuerID - Unique hash ID of rescuer\n",
    "- VideoAmt - Total uploaded videos for this pet\n",
    "- PhotoAmt - Total uploaded photos for this pet\n",
    "- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n",
    "- AdoptionSpeed Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n",
    "    0 - Pet was adopted on the same day as it was listed. \n",
    "    1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n",
    "    2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n",
    "    3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n",
    "    4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n",
    "\n",
    "### Images\n",
    "\n",
    "For pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked.\n",
    "\n",
    "### Image Metadata\n",
    "We have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n",
    "\n",
    "File name format is PetID-ImageNumber.json.\n",
    "\n",
    "Some properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation.\n",
    "\n",
    "Google Vision API reference: https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate\n",
    "\n",
    "### Sentiment Data\n",
    "We have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.\n",
    "\n",
    "File name format is PetID.json.\n",
    "\n",
    "Google Natural Language API reference: https://cloud.google.com/natural-language/docs/basics\n",
    "\n",
    "What will change in the 2nd stage of the competition?\n",
    "In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data:\n",
    "\n",
    "test.zip including test.csv and sample_submission.csv\n",
    "test_images.zip\n",
    "test_metadata.zip\n",
    "test_sentiment.zip\n",
    "\n",
    "In stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training data CSV to pandas dataframe, marking categorical columns as category type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_pandas(is_train=True):\n",
    "    path = os.path.join(INPUT_DIR, 'train', 'train.csv') if is_train else os.path.join(INPUT_DIR, 'test', 'test.csv')\n",
    "    return pd.read_csv(path, dtype={\n",
    "        'Type': 'category',\n",
    "        'Name': 'category',\n",
    "        'Breed1': 'category',\n",
    "        'Breed2': 'category',\n",
    "        'Gender': 'category',\n",
    "        'Color1': 'category',\n",
    "        'Color2': 'category',\n",
    "        'Color3': 'category',\n",
    "        'MaturitySize': 'category',\n",
    "        'FurLength': 'category',\n",
    "        'Vaccinated': 'category',\n",
    "        'Dewormed': 'category',\n",
    "        'Sterilized': 'category',\n",
    "        'Health': 'category',\n",
    "        'State': 'category',\n",
    "        'RescuerID': 'category'\n",
    "    })\n",
    "\n",
    "train_df = read_csv_to_pandas(is_train=True)\n",
    "X_test = read_csv_to_pandas(is_train=False)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training and validation set\n",
    "We split `train_df` into two sets. `X_train` is used for the cross-validation, `X_val` is used at the end of the notebook to estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def to_features_and_labels(df):\n",
    "    y = df['AdoptionSpeed'].values\n",
    "    X = df.drop('AdoptionSpeed', axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_train_val, y_train_val = to_features_and_labels(train_df)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42,\n",
    "                                                  stratify=y_train_val)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformers\n",
    "We'll use [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to define our data preprocessing transforms. We'll use a few custom transformers for the purpose:\n",
    "- `DataFrameColumnMapper`: Map DataFrame column to a new column (similar to `DataFrameMapper` from [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas))\n",
    "- `CategoricalTruncator`: Keep only `N` most frequent categories for a given column, replace others with \"Other\"\n",
    "- `CategoricalOneHotEncoder`: One-hot encode columns\n",
    "- `DataFrameColumnDropper`: Drop given columns\n",
    "- `ColumnByFeatureImportancePicker`: Pick `N` most important columns based on a classifier feature importance\n",
    "- `DataFrameToValuesTransformer`: Map DataFrame to NumPy array, used before predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameColumnMapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Map DataFrame column to a new column (similar to DataFrameMapper from sklearn-pandas)\n",
    "    \n",
    "    Attributes:\n",
    "        column_name (str): Column name to transform\n",
    "        mapping_func (func): Function to apply to given column values\n",
    "        new_column_name (str): Name for the new column, leave empty if replacing `column_name`\n",
    "        drop_original (bool): Drop original column if true and new_column_name != column_name\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, mapping_func, new_column_name=None, drop_original=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.column_name = column_name\n",
    "        self.mapping_func = mapping_func\n",
    "        self.new_column_name = new_column_name if new_column_name is not None else self.column_name\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_column = X.transform({self.column_name: self.mapping_func})\n",
    "        Y = X.copy()\n",
    "        Y = Y.assign(**{self.new_column_name: transformed_column})\n",
    "        if self.column_name != self.new_column_name and self.drop_original:\n",
    "            Y = Y.drop(self.column_name, axis=1)\n",
    "        return Y\n",
    "\n",
    "class CategoricalToOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    One-hot encode given columns.\n",
    "    \n",
    "    Attributes:\n",
    "        columns (List[str]): Columns to one-hot encode.\n",
    "        mappings_ (Dict[str, Dict]): Mapping from original column name to the one-hot-encoded column names\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.mappings_ = None\n",
    "    def fit(self, X, y=None):\n",
    "        # Pick all categorical attributes if no columns to transform were specified\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(exclude='number')\n",
    "        \n",
    "        # Keep track of which categorical attributes are assigned to which integer. This is important \n",
    "        # when transforming the test set.\n",
    "        mappings = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            labels, uniques = X.loc[:, col].factorize() # Assigns unique integers for all categories\n",
    "            int_and_cat = list(enumerate(uniques))\n",
    "            cat_and_int = [(x[1], x[0]) for x in int_and_cat]\n",
    "            mappings[col] = {'int_to_cat': dict(int_and_cat), 'cat_to_int': dict(cat_and_int)}\n",
    "    \n",
    "        self.mappings_ = mappings\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Y = X.copy()\n",
    "        for col in self.columns:\n",
    "            transformed_col = Y.loc[:, col].transform(lambda x: self.mappings_[col]['cat_to_int'][x])\n",
    "            for key, val in self.mappings_[col]['cat_to_int'].items():\n",
    "                one_hot = (transformed_col == val) + 0 # Cast boolean to int by adding zero\n",
    "                Y = Y.assign(**{'{}_{}'.format(col, key): one_hot})\n",
    "            Y = Y.drop(col, axis=1)\n",
    "        return Y\n",
    "\n",
    "class CategoricalTruncator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Keep only N most frequent categories for a given column, replace others with \"Other\"\n",
    "    \n",
    "    Attributes:\n",
    "        column_name (str): Column for which to truncate categories\n",
    "        n_values_to_keep (int): How many of the most frequent values to keep (1 for keeping only most frequent, etc.)\n",
    "        values_ (List[str]): List of category names to keep, others are replaced with \"Other\"\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name, n_values_to_keep=5):\n",
    "        self.column_name = column_name\n",
    "        self.n_values_to_keep = n_values_to_keep\n",
    "        self.values_ = None\n",
    "    def fit(self, X, y=None):\n",
    "        # Here we must ensure that the test set is transformed similarly in the later phase and that the same values are kept\n",
    "        self.values_ = list(X[self.column_name].value_counts()[:self.n_values_to_keep].keys())\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        transform = lambda x: x if x in self.values_ else 'Other'\n",
    "        Y = X.copy()\n",
    "        y = Y.transform({self.column_name: transform})\n",
    "        return Y.assign(**{self.column_name: y})\n",
    "\n",
    "class DataFrameColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Drop given columns.\n",
    "    \n",
    "    Attributes:\n",
    "        column_names (List[Str]): List of columns to drop\n",
    "    \"\"\"\n",
    "    def __init__(self, column_names):\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.copy().drop(self.column_names, axis=1)\n",
    "\n",
    "class DataFrameToValuesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform DataFrame to NumPy array.\n",
    "    \n",
    "    Attributes:\n",
    "        attributes_ (List[str]): List of DataFrame column names\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.attributes_ = None\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        # Remember the order of attributes before converting to NumPy to ensure the columns\n",
    "        # are included in the same order when transforming validation or test dataset\n",
    "        self.attributes_ = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attributes_].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocessing pipeline\n",
    "Build [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) mapping `X_train` to `X_train_preprocessed`. Note that we **never** modify `X_train`: This ensures that our results are independendent of the order in which cells are executed (as long as all variables and functions are defined).\n",
    "\n",
    "Because we're using CatBoost that can handle categorical features quite well, we don't do any other preprocessing than drop some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_pipeline() -> Pipeline:\n",
    "     return Pipeline([\n",
    "        ('drop_unused_columns', DataFrameColumnDropper(\n",
    "            column_names=['PetID', 'Description', 'RescuerID', 'Name'])\n",
    "        )\n",
    "     ])\n",
    "\n",
    "preprocessing_pipeline = build_preprocessing_pipeline()\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "X_train_preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\", len(list(X_train_preprocessed)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Numerical columns:\", list(X_train_preprocessed.select_dtypes(include=\"number\")))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Categorical columns:\", list(X_train_preprocessed.select_dtypes(include=\"category\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost classifier\n",
    "We define a small helper class called `CatBoostPandasClassifier` for training `CatBoostClassifier` with Pandas dataframes. The helper class correctly passes the categorical columns as `cat_features` to CatBoostClassifier's `fit` method and ensures that the order of the columns in fitting and prediction phases matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "class CatBoostPandasClassifier:\n",
    "    \"\"\"\n",
    "    Helper class for training `CatBoostClassifier` with Pandas dataframes. The class\n",
    "    passes the columns of type \"category\" as `cat_features` to `CatBoostClassifier`'s fit method and ensures\n",
    "    that the order of the columns in the fitting and prediction phases matches.\n",
    "    \n",
    "    Author: Kimmo SÃ¤Ã¤skilahti\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.catboost_classifier = CatBoostClassifier(*args, **kwargs)\n",
    "        self.columns = None\n",
    "        \n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        self.columns = list(X)\n",
    "        cat_columns = list(X.select_dtypes(include='category'))\n",
    "        cat_features = [X.columns.get_loc(name) for name in cat_columns]  # Indices of categorical features\n",
    "        return self.catboost_classifier.fit(X.values, y, *args, cat_features=cat_features, **kwargs)\n",
    "        \n",
    "    def copy(self, *args, **kwargs):\n",
    "        returned_classifier = CatBoostPandasClassifier()\n",
    "        returned_classifier.catboost_classifier = self.catboost_classifier.copy()\n",
    "        returned_classifier.columns = self.columns\n",
    "        return returned_classifier\n",
    "        \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        X_copy = X.loc[:, self.columns]\n",
    "        return self.catboost_classifier.predict(X_copy.values, *args, **kwargs)\n",
    "    \n",
    "    def predict_proba(self, X, *args, **kwargs):\n",
    "        X_copy = X.loc[:, self.columns]\n",
    "        return self.catboost_classifier.predict_proba(X_copy.values, *args, **kwargs)\n",
    "        \n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"\n",
    "        Pass all other method calls to self.catboost_classifier.\n",
    "        \"\"\"\n",
    "        return getattr(self.catboost_classifier, attr)\n",
    "\n",
    "# Example usage\n",
    "catboost_pandas_clf = CatBoostPandasClassifier(iterations=10, learning_rate=0.1, loss_function='MultiClass',\n",
    "                                               allow_writing_files=False)\n",
    "# catboost_pandas_clf.fit(X_train_preprocessed, y_train)\n",
    "cross_val_score(catboost_pandas_clf, X_train_preprocessed, y_train, cv=5, scoring=QUADRATIC_WEIGHT_SCORER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define a few helper functions to help in parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search(pipeline, param_distributions, n_iter=10):\n",
    "    \"\"\"\n",
    "    Builder function for RandomizedSearch.\n",
    "    \"\"\"\n",
    "    return RandomizedSearchCV(pipeline, param_distributions=param_distributions, \n",
    "                              cv=5, return_train_score=True, refit='cohen_kappa_quadratic',\n",
    "                              n_iter=n_iter,\n",
    "                              n_jobs=None,\n",
    "                              scoring={\n",
    "                                    'accuracy': make_scorer(accuracy_score),\n",
    "                                    'cohen_kappa_quadratic': QUADRATIC_WEIGHT_SCORER\n",
    "                               },\n",
    "                              verbose=1,\n",
    "                              random_state=42)\n",
    "\n",
    "def pretty_cv_results(cv_results, \n",
    "                      sort_by='rank_test_cohen_kappa_quadratic',\n",
    "                      sort_ascending=True,\n",
    "                      n_rows=10):\n",
    "    \"\"\"\n",
    "    Return pretty Pandas dataframe from the `cv_results_` attribute of finished parameter search,\n",
    "    ranking by test performance and only keeping the columns of interest.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    cols_of_interest = [key for key in df.keys() if key.startswith('param_') \n",
    "                        or key.startswith('mean_train') \n",
    "                        or key.startswith('mean_test_')\n",
    "                        or key.startswith('mean_fit_time')\n",
    "                        or key.startswith('rank')]\n",
    "    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n",
    "\n",
    "def run_search(search):\n",
    "    search.fit(X_train, y_train)\n",
    "    print('Best score is:', search.best_score_)\n",
    "    return pretty_cv_results(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune CatBoostPandasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"GPU\" if GPU_AVAILABLE else \"CPU\"\n",
    "\n",
    "catboost_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('classifier', CatBoostPandasClassifier(verbose=0, loss_function='MultiClass',\n",
    "                                            allow_writing_files=False, task_type=task_type))\n",
    "])\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__iterations': [500],  # Sets the value of `iterations` to the pipeline step `classifier` in parameter search\n",
    "    'classifier__learning_rate': [0.03, 0.10, 0.20],\n",
    "    'classifier__max_depth': [4, 6, 8],\n",
    "    'classifier__one_hot_max_size': [10, 30],\n",
    "    'classifier__l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "catboost_search = build_search(catboost_pipeline, param_distributions=param_distributions, n_iter=10)\n",
    "catboost_cv_results = run_search(search=catboost_search)\n",
    "catboost_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = catboost_search.best_estimator_\n",
    "best_estimator.fit(X_train, y_train)\n",
    "y_val_pred = best_estimator.predict(X_val)\n",
    "\n",
    "print(\"Performance of best estimator on the hold-out set:\", cohen_kappa_score(y_val, y_val_pred, weights='quadratic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the final estimator with all data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(estimator, X):\n",
    "    predictions = estimator.predict(X).astype(np.int32)\n",
    "    predictions = np.squeeze(predictions)  # Estimators may return arrays for each prediction\n",
    "    indices = X.loc[:, 'PetID']\n",
    "    as_dict = [{'PetID': index, 'AdoptionSpeed': prediction} for index, prediction in zip(indices, predictions)]\n",
    "    df = pd.DataFrame.from_dict(as_dict)\n",
    "    df = df.reindex(['PetID', 'AdoptionSpeed'], axis=1)\n",
    "    return df\n",
    "\n",
    "predictions = get_predictions(best_estimator, X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `submission.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission(predictions):\n",
    "    submission_folder = '.'\n",
    "    dest_file = os.path.join(submission_folder, 'submission.csv')\n",
    "    predictions.to_csv(dest_file, index=False)\n",
    "    print(\"Wrote to {}\".format(dest_file))\n",
    "    \n",
    "write_submission(predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
