{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(20.0, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(INPUT_DIR))\n",
    "train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train', 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description (copied from [competition description](https://www.kaggle.com/c/petfinder-adoption-prediction/data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\n",
    "In this competition you will predict the speed at which a pet is adopted, based on the petâ€™s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \n",
    "This is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n",
    "\n",
    "### File descriptions\n",
    "- train.csv - Tabular/text data for the training set\n",
    "- test.csv - Tabular/text data for the test set\n",
    "- sample_submission.csv - A sample submission file in the correct format\n",
    "- breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n",
    "- color_labels.csv - Contains ColorName for each ColorID\n",
    "- state_labels.csv - Contains StateName for each StateID\n",
    "\n",
    "### Data Fields\n",
    "- PetID - Unique hash ID of pet profile\n",
    "- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n",
    "- Type - Type of animal (1 = Dog, 2 = Cat)\n",
    "- Name - Name of pet (Empty if not named)\n",
    "- Age - Age of pet when listed, in months\n",
    "- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n",
    "- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n",
    "- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n",
    "- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n",
    "- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n",
    "- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n",
    "- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n",
    "- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n",
    "- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n",
    "- Quantity - Number of pets represented in profile\n",
    "- Fee - Adoption fee (0 = Free)\n",
    "- State - State location in Malaysia (Refer to StateLabels dictionary)\n",
    "- RescuerID - Unique hash ID of rescuer\n",
    "- VideoAmt - Total uploaded videos for this pet\n",
    "- PhotoAmt - Total uploaded photos for this pet\n",
    "- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n",
    "- AdoptionSpeed Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n",
    "    0 - Pet was adopted on the same day as it was listed. \n",
    "    1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n",
    "    2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n",
    "    3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n",
    "    4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n",
    "\n",
    "### Images\n",
    "\n",
    "For pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked.\n",
    "\n",
    "### Image Metadata\n",
    "We have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n",
    "\n",
    "File name format is PetID-ImageNumber.json.\n",
    "\n",
    "Some properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation.\n",
    "\n",
    "Google Vision API reference: https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate\n",
    "\n",
    "### Sentiment Data\n",
    "We have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.\n",
    "\n",
    "File name format is PetID.json.\n",
    "\n",
    "Google Natural Language API reference: https://cloud.google.com/natural-language/docs/basics\n",
    "\n",
    "What will change in the 2nd stage of the competition?\n",
    "In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data:\n",
    "\n",
    "test.zip including test.csv and sample_submission.csv\n",
    "test_images.zip\n",
    "test_metadata.zip\n",
    "test_sentiment.zip\n",
    "\n",
    "In stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we'll do here  for each column\n",
    "- `PetId`: Keep for reference, drop for prediction\n",
    "- `Type`: One-hot encode into `isCat` or `isDog` field\n",
    "- `Name`: Create a field for if name exists or not, drop *Name* column\n",
    "- `Age`: Leave as is\n",
    "- `Breed1`: **Not considered in this kernel, drop**\n",
    "- `Breed2`: **Not considered in this kernel, drop**\n",
    "- `Gender`: One-hot encode\n",
    "- `Color1`, `Color2`, `Color3`: One-hot encode *Color1*, drop others\n",
    "- `MaturitySize`: One-hot encode, accounting for zero\n",
    "- `FurLength`: One-hot encode, accounting for zero\n",
    "- `Vaccinated`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Dewormed`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Sterilized`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Health`: One-hot encode, accounting for 0 (not specified)\n",
    "- `Quantity`: Leave as is\n",
    "- `Fee`: Leave as is\n",
    "- `State`: **Not considered in this kernel, drop**\n",
    "- `VideoAmt`, `PhotosAmt`: Leave as is\n",
    "- `Description`: Leave as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick look at the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transforms\n",
    "We'll use [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to define our data preprocessing transforms. We'll use a few custom transformers for the purpose:\n",
    "- `DataFrameColumnMapper`: Maps DataFrame columns to a new column (similar to `DataFrameMapper` from `sklearn-pandas`)\n",
    "- `CategoricalOneHotEncoder`: One-hot encodes categorical columns\n",
    "- `DataFrameColumnDropper`: Drops given columns\n",
    "- `DataFrameToValuesTransformer`: Maps DataFrame to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameColumnMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, mapping_func, new_column_name=None, drop_original=True):\n",
    "        self.column_name = column_name\n",
    "        self.mapping_func = mapping_func\n",
    "        self.new_column_name = new_column_name if new_column_name is not None else self.column_name\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_column = X.transform({self.column_name: self.mapping_func})\n",
    "        Y = X.copy()\n",
    "        Y = Y.assign(**{self.new_column_name: transformed_column})\n",
    "        if self.column_name != self.new_column_name and self.drop_original:\n",
    "            Y = Y.drop(self.column_name, axis=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalToOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        # Pick all categorical attributes if no columns to transform were specified\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(exclude='number')\n",
    "        \n",
    "        # Keep track of which categorical attributes are assigned to which integer. This is important \n",
    "        # when transforming the test set.\n",
    "        mappings = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            labels, uniques = X.loc[:, col].factorize() # Assigns unique integers for all categories\n",
    "            int_and_cat = list(enumerate(uniques))\n",
    "            cat_and_int = [(x[1], x[0]) for x in int_and_cat]\n",
    "            mappings[col] = {'int_to_cat': dict(int_and_cat), 'cat_to_int': dict(cat_and_int)}\n",
    "    \n",
    "        self.mappings = mappings\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Y = X.copy()\n",
    "        for col in self.columns:\n",
    "            transformed_col = Y.loc[:, col].transform(lambda x: self.mappings[col]['cat_to_int'][x])\n",
    "            for key, val in self.mappings[col]['cat_to_int'].items():\n",
    "                one_hot = (transformed_col == val) + 0 # Cast boolean to int by adding zero\n",
    "                Y = Y.assign(**{'{}_{}'.format(col, key): one_hot})\n",
    "            Y = Y.drop(col, axis=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_names):\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.copy().drop(self.column_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameToValuesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        # Remember the order of attributes before converting to NumPy to ensure the columns\n",
    "        # are included in the same order when transforming validation or test dataset\n",
    "        self.attribute_order = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attribute_order].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def drop_unused_columns(df):\n",
    "    return df.drop(['Breed1', 'Breed2', 'State'], axis=1)\n",
    "\n",
    "def to_features_and_labels(df):\n",
    "    y = df['AdoptionSpeed'].values\n",
    "    X = drop_unused_columns(df)\n",
    "    X = X.drop('AdoptionSpeed', axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_train_val, y_train_val = to_features_and_labels(train_df) # All data with labels, to be split into train and val\n",
    "\n",
    "# Split the available training data into training set and validation set (used for estimating the generalization error)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42,\n",
    "                                                  stratify=y_train_val)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import FunctionTransformer, DataFrameMapper\n",
    "\n",
    "def has_field_transformer(column_name) -> DataFrameMapper:\n",
    "    return DataFrameMapper(features=[\n",
    "                                (column_name, FunctionTransformer(lambda name: np.int(pd.notna(name))))\n",
    "                           ], \n",
    "                           default=None,\n",
    "                           df_out=True)\n",
    "\n",
    "def map_categories(column_name, mapping_dict) -> DataFrameMapper:\n",
    "    return DataFrameMapper(features=[\n",
    "                                (column_name, FunctionTransformer(lambda x: mapping_dict[x]))\n",
    "                           ], \n",
    "                           default=None,\n",
    "                           df_out=True)\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline() -> Pipeline:\n",
    "     return Pipeline([\n",
    "        ('has_name', has_field_transformer(column_name=\"Name\")),\n",
    "        ('type_to_species', map_categories(column_name=\"Type\", mapping_dict={1: 'Dog', 2: 'Cat'})),\n",
    "        ('gender_to_names', map_categories(column_name=\"Gender\", mapping_dict={1: 'male', 2: 'female', 3: 'mixed'})),\n",
    "        ('type_onehot', CategoricalToOneHotEncoder(columns=[\"Type\"])),\n",
    "        ('gender_onehot', CategoricalToOneHotEncoder(columns=[\"Gender\"])),\n",
    "        ('color_onehot', CategoricalToOneHotEncoder(columns=[\"Color1\"])),\n",
    "        ('maturity_size_onehot', CategoricalToOneHotEncoder(columns=[\"MaturitySize\"])),\n",
    "        ('health_onehot', CategoricalToOneHotEncoder(columns=[\"Health\"])),\n",
    "        ('furlength_onehot', CategoricalToOneHotEncoder(columns=[\"FurLength\"])),\n",
    "        ('vaccinated_onehot', CategoricalToOneHotEncoder(columns=[\"Vaccinated\"])),\n",
    "        ('dewormed_onehot', CategoricalToOneHotEncoder(columns=[\"Dewormed\"])),\n",
    "        ('sterilized_onehot', CategoricalToOneHotEncoder(columns=[\"Sterilized\"])),\n",
    "        ('drop_columns', DataFrameColumnDropper(\n",
    "            column_names=['PetID', 'Description', 'RescuerID', 'Color2', 'Color3', 'Type_Dog']))\n",
    "    ])\n",
    "\n",
    "preprocessing_pipeline = build_preprocessing_pipeline()\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessing_pipeline.transform(X_val)\n",
    "\n",
    "X_train_preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", [(column_name, str(X_train_preprocessed[column_name].dtype))\n",
    "                   for column_name in list(X_train_preprocessed)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preparation_pipeline():\n",
    "    return Pipeline([\n",
    "        ('to_numpy', DataFrameToValuesTransformer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "def build_full_pipeline(classifier=None):\n",
    "    preprocessing_pipeline = build_preprocessing_pipeline()\n",
    "    preparation_pipeline = build_preparation_pipeline()\n",
    "    return Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('prepare', preparation_pipeline),\n",
    "        ('classifier', classifier) # Expected to be filled by grid search\n",
    "    ])\n",
    "\n",
    "def build_grid_search(pipeline, param_grid):\n",
    "    return GridSearchCV(pipeline, param_grid, cv=5, return_train_score=True, refit='accuracy',\n",
    "                        scoring={\n",
    "                                    'accuracy': make_scorer(accuracy_score)\n",
    "                                },\n",
    "                        verbose=1)\n",
    "\n",
    "def pretty_cv_results(cv_results, \n",
    "                      sort_by='rank_test_accuracy',\n",
    "                      sort_ascending=True,\n",
    "                      n_rows=5):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    cols_of_interest = [key for key in df.keys() if key.startswith('param_') \n",
    "                        or key.startswith('mean_train') \n",
    "                        or key.startswith('mean_test_')\n",
    "                        or key.startswith('rank')]\n",
    "    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n",
    "\n",
    "def run_grid_search(grid_search):\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Best test score accuracy is:', grid_search.best_score_)\n",
    "    return pretty_cv_results(grid_search.cv_results_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier()]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = build_grid_search(pipeline=build_full_pipeline(), param_grid=param_grid)\n",
    "rf_cv = run_grid_search(grid_search=log_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
