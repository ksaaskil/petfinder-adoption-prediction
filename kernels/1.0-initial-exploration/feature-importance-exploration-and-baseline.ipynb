{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(20.0, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(INPUT_DIR))\n",
    "train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train', 'train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(INPUT_DIR, 'test', 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description (copied from [competition description](https://www.kaggle.com/c/petfinder-adoption-prediction/data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\n",
    "In this competition you will predict the speed at which a pet is adopted, based on the petâ€™s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \n",
    "This is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n",
    "\n",
    "### File descriptions\n",
    "- train.csv - Tabular/text data for the training set\n",
    "- test.csv - Tabular/text data for the test set\n",
    "- sample_submission.csv - A sample submission file in the correct format\n",
    "- breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n",
    "- color_labels.csv - Contains ColorName for each ColorID\n",
    "- state_labels.csv - Contains StateName for each StateID\n",
    "\n",
    "### Data Fields\n",
    "- PetID - Unique hash ID of pet profile\n",
    "- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n",
    "- Type - Type of animal (1 = Dog, 2 = Cat)\n",
    "- Name - Name of pet (Empty if not named)\n",
    "- Age - Age of pet when listed, in months\n",
    "- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n",
    "- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n",
    "- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n",
    "- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n",
    "- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n",
    "- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n",
    "- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n",
    "- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n",
    "- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n",
    "- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n",
    "- Quantity - Number of pets represented in profile\n",
    "- Fee - Adoption fee (0 = Free)\n",
    "- State - State location in Malaysia (Refer to StateLabels dictionary)\n",
    "- RescuerID - Unique hash ID of rescuer\n",
    "- VideoAmt - Total uploaded videos for this pet\n",
    "- PhotoAmt - Total uploaded photos for this pet\n",
    "- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n",
    "- AdoptionSpeed Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n",
    "    0 - Pet was adopted on the same day as it was listed. \n",
    "    1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n",
    "    2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n",
    "    3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n",
    "    4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n",
    "\n",
    "### Images\n",
    "\n",
    "For pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked.\n",
    "\n",
    "### Image Metadata\n",
    "We have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n",
    "\n",
    "File name format is PetID-ImageNumber.json.\n",
    "\n",
    "Some properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation.\n",
    "\n",
    "Google Vision API reference: https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate\n",
    "\n",
    "### Sentiment Data\n",
    "We have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.\n",
    "\n",
    "File name format is PetID.json.\n",
    "\n",
    "Google Natural Language API reference: https://cloud.google.com/natural-language/docs/basics\n",
    "\n",
    "What will change in the 2nd stage of the competition?\n",
    "In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data:\n",
    "\n",
    "test.zip including test.csv and sample_submission.csv\n",
    "test_images.zip\n",
    "test_metadata.zip\n",
    "test_sentiment.zip\n",
    "\n",
    "In stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we'll do here  for each column\n",
    "- `PetId`: Keep for reference, drop for prediction\n",
    "- `Type`: One-hot encode into `isCat` or `isDog` field\n",
    "- `Name`: Create a field for if name exists or not, drop *Name* column\n",
    "- `Age`: Leave as is\n",
    "- `Breed1`: Keep `N` most frequent categories, one-hot encode\n",
    "- `Breed2`: Keep `N` most frequent categories, one-hot encode\n",
    "- `Gender`: One-hot encode\n",
    "- `Color1`, `Color2`, `Color3`: One-hot encode *Color1*, drop others\n",
    "- `MaturitySize`: One-hot encode, accounting for zero\n",
    "- `FurLength`: One-hot encode, accounting for zero\n",
    "- `Vaccinated`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Dewormed`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Sterilized`: One-hot encode, accounting for 3 (not sure)\n",
    "- `Health`: One-hot encode, accounting for 0 (not specified)\n",
    "- `Quantity`: Leave as is\n",
    "- `Fee`: Leave as is\n",
    "- `State`: Keep `N` most frequent categories, one-hot encode\n",
    "- `VideoAmt`, `PhotosAmt`: Leave as is\n",
    "- `Description`: Leave as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick look at the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transforms\n",
    "We'll use [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to define our data preprocessing transforms. We'll use a few custom transformers for the purpose:\n",
    "- `DataFrameColumnMapper`: Maps DataFrame columns to a new column (similar to `DataFrameMapper` from `sklearn-pandas`)\n",
    "- `CategoricalOneHotEncoder`: One-hot encodes categorical columns\n",
    "- `DataFrameColumnDropper`: Drops given columns\n",
    "- `DataFrameToValuesTransformer`: Maps DataFrame to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameColumnMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, mapping_func, new_column_name=None, drop_original=True):\n",
    "        self.column_name = column_name\n",
    "        self.mapping_func = mapping_func\n",
    "        self.new_column_name = new_column_name if new_column_name is not None else self.column_name\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_column = X.transform({self.column_name: self.mapping_func})\n",
    "        Y = X.copy()\n",
    "        Y = Y.assign(**{self.new_column_name: transformed_column})\n",
    "        if self.column_name != self.new_column_name and self.drop_original:\n",
    "            Y = Y.drop(self.column_name, axis=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalToOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        # Pick all categorical attributes if no columns to transform were specified\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(exclude='number')\n",
    "        \n",
    "        # Keep track of which categorical attributes are assigned to which integer. This is important \n",
    "        # when transforming the test set.\n",
    "        mappings = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            labels, uniques = X.loc[:, col].factorize() # Assigns unique integers for all categories\n",
    "            int_and_cat = list(enumerate(uniques))\n",
    "            cat_and_int = [(x[1], x[0]) for x in int_and_cat]\n",
    "            mappings[col] = {'int_to_cat': dict(int_and_cat), 'cat_to_int': dict(cat_and_int)}\n",
    "    \n",
    "        self.mappings = mappings\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Y = X.copy()\n",
    "        for col in self.columns:\n",
    "            transformed_col = Y.loc[:, col].transform(lambda x: self.mappings[col]['cat_to_int'][x])\n",
    "            for key, val in self.mappings[col]['cat_to_int'].items():\n",
    "                one_hot = (transformed_col == val) + 0 # Cast boolean to int by adding zero\n",
    "                Y = Y.assign(**{'{}_{}'.format(col, key): one_hot})\n",
    "            Y = Y.drop(col, axis=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTruncator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, n_values_to_keep=5):\n",
    "        self.column_name = column_name\n",
    "        self.n_values_to_keep = n_values_to_keep\n",
    "        self.values = None\n",
    "    def fit(self, X, y=None):\n",
    "        # Here we must ensure that the test set is transformed similarly in the later phase and that the same values are kept\n",
    "        self.values = list(X[self.column_name].value_counts()[:self.n_values_to_keep].keys())\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        transform = lambda x: x if x in self.values else 'Other'\n",
    "        y = X.transform({self.column_name: transform})\n",
    "        return X.assign(**{self.column_name: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_names):\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.copy().drop(self.column_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameToValuesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.attributes_ = None\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        # Remember the order of attributes before converting to NumPy to ensure the columns\n",
    "        # are included in the same order when transforming validation or test dataset\n",
    "        self.attributes_ = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attributes_].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def to_features_and_labels(df):\n",
    "    y = df['AdoptionSpeed'].values\n",
    "    X = df.drop('AdoptionSpeed', axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_train_val, y_train_val = to_features_and_labels(train_df) # All data with labels, to be split into train and val\n",
    "\n",
    "# Split the available training data into training set and validation set (used for estimating the generalization error)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42,\n",
    "                                                  stratify=y_train_val)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_field_transformer(column_name, new_column_name=None, is_missing_func=pd.notna) -> TransformerMixin:\n",
    "    return DataFrameColumnMapper(column_name=column_name,\n",
    "                                 mapping_func=lambda name: np.int(is_missing_func(name)),\n",
    "                                 drop_original=True,\n",
    "                                 new_column_name=new_column_name if new_column_name is not None else column_name)\n",
    "\n",
    "def value_matches_transformer(column_name, new_column_name=None, matches=pd.notna) -> TransformerMixin:\n",
    "    return DataFrameColumnMapper(column_name=column_name,\n",
    "                                 mapping_func=lambda value: np.int(matches(value)),\n",
    "                                 drop_original=False,\n",
    "                                 new_column_name=new_column_name if new_column_name is not None else column_name)\n",
    "\n",
    "def map_categories(column_name, mapping_dict) -> TransformerMixin:\n",
    "    return DataFrameColumnMapper(column_name=column_name,\n",
    "                                 mapping_func=lambda x: mapping_dict[x])\n",
    "\n",
    "def onehot_encode(columns) -> TransformerMixin:\n",
    "    return CategoricalToOneHotEncoder(columns=columns)\n",
    "\n",
    "def truncate_categorical(column_name, n_values_to_keep=10):\n",
    "    return CategoricalTruncator(column_name=column_name, n_values_to_keep=n_values_to_keep)\n",
    "\n",
    "ONEHOT_ENCODED_COLUMNS = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Health\",\n",
    "                          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\"]\n",
    "\n",
    "\"\"\"\n",
    "def has_photo() -> TransformerMixin:\n",
    "    return DataFrameColumnMapper(column_name=\"PhotoAmt\",\n",
    "                                                mapping_func=lambda value: np.int(value > 0),\n",
    "                                                drop_original=False,\n",
    "                                                new_column_name=\"hasPhoto\")\n",
    "\"\"\"\n",
    "\n",
    "def build_preprocessing_pipeline() -> Pipeline:\n",
    "     return Pipeline([\n",
    "        ('add_has_name', has_field_transformer(column_name=\"Name\", new_column_name=\"hasName\")),\n",
    "        ('add_is_free', value_matches_transformer(column_name=\"Fee\", new_column_name=\"isFree\",\n",
    "                                                  matches=lambda value: value < 1)),\n",
    "        ('map_type_to_species', map_categories(column_name=\"Type\", mapping_dict={1: 'dog', 2: 'cat'})),\n",
    "        ('map_gender_to_names', map_categories(column_name=\"Gender\", mapping_dict={1: 'male', 2: 'female', 3: 'mixed'})),\n",
    "        ('truncate_breed1', truncate_categorical(column_name=\"Breed1\", n_values_to_keep=10)),\n",
    "        ('truncate_breed2', truncate_categorical(column_name=\"Breed2\", n_values_to_keep=10)),\n",
    "        ('truncate_state', truncate_categorical(column_name=\"State\", n_values_to_keep=5)),\n",
    "        ('onehot_encode', CategoricalToOneHotEncoder(columns=ONEHOT_ENCODED_COLUMNS)),\n",
    "        ('drop_unused_columns', DataFrameColumnDropper(\n",
    "            column_names=['PetID', 'Description', 'RescuerID', 'Color2', 'Color3', 'Type_dog'\n",
    "        ]))\n",
    "    ])\n",
    "\n",
    "preprocessing_pipeline = build_preprocessing_pipeline()\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessing_pipeline.transform(X_val)\n",
    "\n",
    "X_train_preprocessed.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\", len(list(X_train_preprocessed)))\n",
    "print(\"\")\n",
    "\n",
    "# print(\"Columns:\", [(column_name, str(X_train_preprocessed[column_name].dtype))\n",
    "#                    for column_name in list(X_train_preprocessed)])\n",
    "print(\"Numerical columns:\", list(X_train_preprocessed.select_dtypes(include=\"number\")))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Non-numerical columns:\", list(X_train_preprocessed.select_dtypes(exclude=\"number\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that only numerical fields exist in the preprocessed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define helper factory functions for building pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preparation_pipeline():\n",
    "    return Pipeline([\n",
    "        ('to_numpy', DataFrameToValuesTransformer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "def build_full_pipeline(classifier=None):\n",
    "    preprocessing_pipeline = build_preprocessing_pipeline()\n",
    "    preparation_pipeline = build_preparation_pipeline()\n",
    "    return Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('preparation', preparation_pipeline),\n",
    "        ('classifier', classifier)  # Expected to be filled by grid search\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze feature importance\n",
    "Train ``RandomForestClassifier`` and list ``feature_importances_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_pipeline = build_full_pipeline(classifier=rf_classifier)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "feature_names = rf_pipeline.named_steps['preparation'].named_steps['to_numpy'].attributes_\n",
    "\n",
    "feature_importances_with_names = [(feature_name, feature_importance) for feature_name, feature_importance in zip(feature_names, feature_importances)]\n",
    "\n",
    "feature_importances_with_names.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "N_MOST_IMPORTANT_TO_SHOW = 50\n",
    "print(\"Feature importances (top {}):\".format(N_MOST_IMPORTANT_TO_SHOW))\n",
    "for feature_name, feature_importance in feature_importances_with_names[:N_MOST_IMPORTANT_TO_SHOW]:\n",
    "    print(\"{} -> {}\".format(feature_name, feature_importance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_pipeline = build_full_pipeline(classifier=rf_classifier)\n",
    "cross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring=make_scorer(cohen_kappa_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the test set is clearly quite bad. Let us do some grid search to see how much we can improve.\n",
    "First define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid_search(pipeline, param_grid):\n",
    "    return GridSearchCV(pipeline, param_grid, cv=5, return_train_score=True, refit='cohen_kappa',\n",
    "                        scoring={\n",
    "                                    'accuracy': make_scorer(accuracy_score),\n",
    "                                    'cohen_kappa': make_scorer(cohen_kappa_score)\n",
    "                                },\n",
    "                        verbose=2)\n",
    "\n",
    "def pretty_cv_results(cv_results, \n",
    "                      sort_by='rank_test_cohen_kappa',\n",
    "                      sort_ascending=True,\n",
    "                      n_rows=5):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    cols_of_interest = [key for key in df.keys() if key.startswith('param_') \n",
    "                        or key.startswith('mean_train') \n",
    "                        or key.startswith('mean_test_')\n",
    "                        or key.startswith('rank')]\n",
    "    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n",
    "\n",
    "def run_grid_search(grid_search):\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Best test score accuracy is:', grid_search.best_score_)\n",
    "    return pretty_cv_results(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run grid search for a given parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier(n_estimators=100)],\n",
    "        'classifier__n_estimators': [30, 100, 300],\n",
    "        'classifier__max_features': ['auto', 'log2', None]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = build_grid_search(build_full_pipeline(), param_grid=param_grid)\n",
    "run_grid_search(grid_search=grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(grid_search.best_estimator_, X=X_train, y=y_train, cv=5)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true=y_train, y_pred=y_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=range(0, 5),\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gaussian process classifier](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "param_grid = [\n",
    "    { \n",
    "        'classifier': [ GaussianProcessClassifier() ], \n",
    "        'classifier__kernel': [1.0*RBF(1.0), 1.0*Matern(1.0)]\n",
    "    }\n",
    "]\n",
    "\n",
    "gp_grid_search = build_grid_search(pipeline=build_full_pipeline(), param_grid=param_grid)\n",
    "gp_cv_results = run_grid_search(grid_search=gp_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = [\n",
    "    { \n",
    "        'classifier': [ GradientBoostingClassifier(random_state=42) ],\n",
    "        'classifier__loss': ['deviance'],\n",
    "        'classifier__n_estimators': [50, 100, 500],\n",
    "        'classifier__max_features': ['auto', None, 'log2'],\n",
    "        'classifier__max_depth': [3, 5],\n",
    "        'classifier__min_samples_leaf': [1],\n",
    "        'classifier__min_samples_split': [2]\n",
    "    }\n",
    "]\n",
    "\n",
    "gb_grid_search = build_grid_search(pipeline=build_full_pipeline(), param_grid=param_grid)\n",
    "gb_cv_results = run_grid_search(grid_search=gb_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Create a voting classifier from the best estimators and check the generalization accuracy for heldout data X_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_estimators = [\n",
    "    ('rf', grid_search),\n",
    "    # ('logistic', log_grid_search),  # TODO\n",
    "    # ('svc', svm_grid_search),\n",
    "    ('gp', gp_grid_search),\n",
    "    # ('ada', ada_grid_search),\n",
    "    ('gb', gb_grid_search),\n",
    "]\n",
    "\n",
    "estimators_with_names = [(name, grid_search.best_estimator_) for name, grid_search in voting_estimators]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=estimators_with_names, voting='soft')\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "y_val_pred = voting_classifier.predict(X_val)\n",
    "\n",
    "cohen_kappa_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train voting classifier with all data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(estimator, X):\n",
    "    predictions = estimator.predict(X)\n",
    "    indices = X.loc[:, 'PetID']\n",
    "    as_dict = [{'PetID': index, 'AdoptionSpeed': prediction} for index, prediction in zip(indices, predictions)]\n",
    "    df = pd.DataFrame.from_dict(as_dict)\n",
    "    df = df.reindex(['PetID', 'AdoptionSpeed'], axis=1)\n",
    "    return df\n",
    "\n",
    "predictions = get_predictions(voting_classifier, X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission(predictions):\n",
    "    submission_folder = '.'\n",
    "    dest_file = os.path.join(submission_folder, 'submission.csv')\n",
    "    predictions.to_csv(dest_file, index=False)\n",
    "    print(\"Wrote to {}\".format(dest_file))\n",
    "    \n",
    "write_submission(predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
